# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 14:47:35 2023

@author: asalehi
"""


'''

Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point
Please Do not Edit this file on Share Point


'''
import os
import random

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

import numpy as np
import json

from statistics import mean

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.ticker as mticker

from scipy.stats import truncnorm
# %% Initializing Data
def init_AQ_grids(in_dir='Data/AQgrid.gpkg'):
    """
    Initializes and processes the AQ (Air Quality) Grids from a GeoPackage file.

    The function performs the following steps:
    1. Loads the AQ Grid GeoDataFrame from the specified file path.
    2. Converts the coordinate reference system (CRS) to EPSG:4326.
    3. Generates a unique 'GRID_KEY' for each grid based on its 'COL' and 'ROW'.
    4. Filters the GeoDataFrame to keep only the 'GRID_KEY' and 'geometry' columns.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing the AQ Grids.
                    Defaults to 'Data/AQgrid.gpkg'.

    Returns:
    - GeoDataFrame: Processed GeoDataFrame containing the AQ Grids with updated CRS,
                    a new 'GRID_KEY', and only the essential columns.
    """
    # Load the AQ Grid GeoDataFrame from the specified GeoPackage file
    AQ_grid_gdf = gpd.read_file(in_dir)
    
    # Convert the GeoDataFrame's coordinate reference system (CRS) to EPSG:4326
    AQ_grid_gdf.crs = "EPSG:4326"
    
    # Generate a 'GRID_KEY' for each grid, combining 'COL' and 'ROW' values
    AQ_grid_gdf['GRID_KEY'] = [str((col, row)) for col, row in zip(AQ_grid_gdf['COL'], AQ_grid_gdf['ROW'])]
    
    # Define the columns to keep in the final GeoDataFrame
    columns_to_keep = ['GRID_KEY', 'geometry']
    
    # Filter the GeoDataFrame to include only the specified columns
    AQ_grid_gdf = AQ_grid_gdf[columns_to_keep]
    
    # Return the processed GeoDataFrame
    return AQ_grid_gdf

def init_climatezone_grid_map(in_dir='Data/AQ_grid_climatezone_IECC2015.gpkg'):
    """
    Initialize a mapping between grid keys and climate zones from a GeoPackage file.

    This function reads a geospatial data file to create a dictionary where each grid key
    corresponds to an International Energy Conservation Code (IECC) climate zone.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing the grid key and IECC climate zone data.

    Returns:
    - dict: A dictionary mapping each grid key to its corresponding IECC climate zone.
    """
    
    # Read the GeoPackage file containing the climate zone data
    climatezone_grid_gdf = gpd.read_file(in_dir)
    
    # Initialize a dictionary to store the grid key to climate zone mapping
    climatezone_grid_map = {}
    
    # Iterate over each row in the geodataframe to populate the mapping dictionary
    for idx, row in climatezone_grid_gdf.iterrows():
        GRID_KEY = row['GRID_KEY']  # Extract the grid key
        IECC = row['IECC15']        # Extract the IECC climate zone
        climatezone_grid_map[GRID_KEY] = IECC  # Assign the IECC zone to the grid key in the dictionary
    
    # Return the completed mapping dictionary
    return climatezone_grid_map

def init_County(in_dir='Data/County_Main_Land.gpkg'):
    """
    Initializes and processes the County geometries from a GeoPackage file.

    The function performs the following steps:
    1. Loads the County GeoDataFrame from the specified file path.
    2. Converts the coordinate reference system (CRS) to EPSG:4326.
    3. Filters the GeoDataFrame to keep only the 'FIPS' and 'geometry' columns.

    Parameters:
    - in_dir (str): Path to the input GeoPackage file containing County geometries.
                    Defaults to 'Data/County_Main_Land.gpkg'.

    Returns:
    - GeoDataFrame: Processed GeoDataFrame containing the County geometries with updated CRS,
                    including only the 'FIPS' code and geometry for each county.
    """
    # Load the County GeoDataFrame from the specified GeoPackage file
    County_gdf = gpd.read_file(in_dir)
    
    # Convert the GeoDataFrame's coordinate reference system (CRS) to EPSG:4326
    County_gdf.crs = "EPSG:4326"
    
    # Define the columns to keep in the final GeoDataFrame
    columns_to_keep = ['FIPS', 'geometry']
    
    # Filter the GeoDataFrame to include only the specified columns
    County_gdf = County_gdf[columns_to_keep]
    
    # Return the processed GeoDataFrame
    return County_gdf


def init_grid_contents_in_county_overlap_mapping(  in_dir = 'Data/Spatial_Grid_contents_in_County_Overlap_Percentages.json' ):
    #This function loads a pre calculated dictionary to map percentage of each county area located in each grid
    with open(in_dir, 'r') as file:
        grid_contents_in_county_overlap_mapping = json.load(file)
    
    return grid_contents_in_county_overlap_mapping

def init_county_contents_in_grid_overlap_mapping(  in_dir = 'Data/Spatial_County_contents_in_Grid_Overlap_Percentages.json' ):
    #This function loads a pre calculated dictionary to map percentage of county contents of a grid
    
    with open(in_dir, 'r') as file:
        county_contents_in_grid_overlap_mapping = json.load(file)

    return county_contents_in_grid_overlap_mapping

def init_PM_Concentrations( in_dir = 'Data/PM_Grid_Daily_Concentration_1981_2010.json'):
    # This Function loads PM2.5 Concentration per Grid Cell per year. 
    #Contains a string of 365 daily concentration
    with open(in_dir, 'r') as file:
        PM_Concentrations_dict = json.load(file)
    
    return PM_Concentrations_dict

def init_PM_Concentrations_means(in_dir = 'Data/PM_Grid_Daily_Mean_Concentration_1981_2010.json'):
    # This Function loads PM2.5 Concentration per Grid Cell per year, and returns the yearly mean value
    with open(in_dir, 'r') as file:
        PM_concentrations_mean_dict = json.load(file)
    
    return PM_concentrations_mean_dict

def start_centuryPM():
    
    PM_dict = init_PM_Concentrations_means()
    # Initialize the new dictionary to hold the reorganized structure
    reorganized_dict = {}
    start_centuryPM_dict = {}
    # Iterate through each level of the nested dictionary
    for year, grid_dict in PM_dict.items():
        for g, value in grid_dict.items():
        
                if g not in reorganized_dict:
                    reorganized_dict[g] = {}
                # Assign the value to the new key structure
                reorganized_dict[g][year] = value
    
    # Return the reorganized dictionary
    for g in reorganized_dict.keys():
        PM_grid = reorganized_dict[g]
        PM_list = []
        for year in PM_grid.keys():
            PM_list.append(PM_grid[year])
            
        PM_average = mean(PM_list)
        start_centuryPM_dict[g] = PM_average
    return start_centuryPM_dict
        


def init_Buildings_Stock( in_dir = 'Data/Buildings_Stock.csv'):
    #This Function Loads the Buildings Stock
    Buildings_Stock_df = pd.read_csv(in_dir)
    return Buildings_Stock_df

def init_ACH50(in_dir = 'Data/ACH50_Grid_Vintage_BT.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        ACH50_dict = json.load(file)
    return ACH50_dict


def init_ACH50_national_mean(in_dir = 'Data/ACH50_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        ACH50_national_mean = json.load(file)
    return ACH50_national_mean

def init_floor_area(in_dir = 'Data/Floorarea_Grid_Vintage_BT.json'):
    #This function initilizes Floor area across grid cells per building type
    
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        fl_area_dict = json.load(file)
    return fl_area_dict

def init_floor_area_national_mean(in_dir = 'Data/Floorarea_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        fl_area_national_mean = json.load(file)
    return fl_area_national_mean

def init_occupants(in_dir = 'Data/Occupants_Grid_Vintage_BT.json'):
    #This function initilizes Floor area across grid cells per building type
    
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        occupants_dict = json.load(file)
    return occupants_dict

def init_occupants_national_mean(in_dir = 'Data/Occupants_national_mean.json'):
    #This function initilizes ACH50 across grid cells per building type
    #For each grid and building type 1000 mean ACH50 occupant based values is provided
    with open(in_dir, 'r') as file:
        occupants_national_mean = json.load(file)
    return occupants_national_mean

test = init_occupants_national_mean()

def reorganized_dict(target_dict):
    """
    Reorganize a nested dictionary to change the hierarchy of the keys.

    This function takes a dictionary with a nested structure and reorganizes it
    such that the innermost keys become the outermost keys in the resulting dictionary.

    Parameters:
    - target_dict (dict): A nested dictionary to reorganize. It's assumed to have
                          a structure where the first level of keys contains values
                          that are also dictionaries, and so on.

    Returns:
    - dict: A reorganized dictionary with the innermost keys as the outermost keys.
    
    Example Structure:
    - Before: target_dict[v][bt][g] -> value
    - After: reorganized_dict[g][v][bt] -> value
    """
    # Initialize the new dictionary to hold the reorganized structure
    reorganized_dict = {}
    
    # Iterate through each level of the nested dictionary
    for v, bt_dict in target_dict.items():
        for bt, g_dict in bt_dict.items():
            for g, value in g_dict.items():
                # Create a new hierarchy with the inner key as the primary key
                if g not in reorganized_dict:
                    reorganized_dict[g] = {}
                if v not in reorganized_dict[g]:
                    reorganized_dict[g][v] = {}
                # Assign the value to the new key structure
                reorganized_dict[g][v][bt] = value
    
    # Return the reorganized dictionary
    return reorganized_dict

def init_ach50_mean(in_dir = 'Data/ACH50_Grid_Vintage_BT.json'):
    #Function Only for deterministic model Not applicable for MCS
    with open(in_dir, 'r') as file:
        ach50 = json.load(file)
    
    national_average = init_ACH50_national_mean()

    ach50_mean = {}
    for v in ach50.keys():
        ach50_mean[v] = {}
        for bt in ach50[v].keys():
            ach50_mean[v][bt] = {}
            for g in ach50[v][bt].keys():
                ach50_mean[v][bt][g] = sum(ach50[v][bt][g]) / len(ach50[v][bt][g])
                if pd.isna(ach50_mean[v][bt][g]):
                    segment = bt + '_' + v
                    ach50_mean[v][bt][g] = national_average[segment]
                
    ach50_mean = reorganized_dict(ach50_mean)
    return ach50_mean

test = init_ach50_mean()

def sample_ach50_mean( ach50 ):
    #Function for MCS
    national_average = init_ACH50_national_mean()

    ach50_mean = {}
    for v in ach50.keys():
        ach50_mean[v] = {}
        for bt in ach50[v].keys():
            ach50_mean[v][bt] = {}
            for g in ach50[v][bt].keys():
                
                ach50_mean[v][bt][g] = random.choice(ach50[v][bt][g])
                
                if pd.isna(ach50_mean[v][bt][g]):
                    segment = bt + '_' + v
                    ach50_mean[v][bt][g] = national_average[segment]
                
    ach50_mean = reorganized_dict(ach50_mean)
    return ach50_mean

def sample_flarea_mean( flarea ):
    #Function for MCS
    national_average = init_floor_area_national_mean()

    flarea_mean = {}
    for v in flarea.keys():
        flarea_mean[v] = {}
        for bt in flarea[v].keys():
            flarea_mean[v][bt] = {}
            for g in flarea[v][bt].keys():
                
                flarea_mean[v][bt][g] = random.choice(flarea[v][bt][g])
                
                if pd.isna(flarea_mean[v][bt][g]):
                    segment = bt + '_' + v
                    flarea_mean[v][bt][g] = national_average[segment]
                
    flarea_mean = reorganized_dict(flarea_mean)
    return flarea_mean

def sample_occupants_mean( occupants ):
    #Function for MCS
    national_average = init_occupants_national_mean()

    occupants_mean = {}
    for v in occupants.keys():
        occupants_mean[v] = {}
        for bt in occupants[v].keys():
            occupants_mean[v][bt] = {}
            for g in occupants[v][bt].keys():
                
                occupants_mean[v][bt][g] = random.choice(occupants[v][bt][g])
                
                if pd.isna(occupants_mean[v][bt][g]):
                    segment = bt + '_' + v
                    occupants_mean[v][bt][g] = national_average[segment]
                
    occupants_mean = reorganized_dict(occupants_mean)
    return occupants_mean


def init_population( in_dir = 'Data/Population.csv'):
    #The Current Sate of The Model only Uses 2000 Population data based on Leuple 
    pop_df = pd.read_csv(in_dir)
    
    pop_dict = {}

    for index, row in pop_df.iterrows():
        population = row['2000']
        col_row_key = (row['COL'], row['ROW'])
        GRID_KEY = str(col_row_key)
        pop_dict[GRID_KEY] = population
        
    return pop_dict

'''
def init_Population_Distribution( in_dir = 'Data/population_buildingtype_grid_percentage_mapping.json' ):
    with open(in_dir, 'r') as file:
        Population_Distribution = json.load(file)
    return Population_Distribution 
'''


def init_Baseline_Mortality( in_dir = 'Data/Baseline_Mortality.json'):
    
    with open(in_dir, 'r') as file:
        Baseline_Mortality_dict = json.load(file)
    return Baseline_Mortality_dict
        
def init_VSL():
    return 11420000
    #return 7000000

def init_Health_Model():
    #This values need to be Corrected
    Baseline_Mortality_dict = init_Baseline_Mortality()
    Relative_Risk = 1.14
    VSL = init_VSL()
    
    return VSL, Relative_Risk, Baseline_Mortality_dict

def init_segment_mapping():
    
    segment_maping = { '<1940' : { 'Multi-Family with 5+ Units': 'MF5P_V1'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V1'
                    , 'Single-Family Attached': 'SF_V1'
                    , 'Single-Family Detached': 'SF_V1'
                    , 'Mobile Home': 'MH_V1' },
                    
                    '1940s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1950s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1960s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1970s' : { 'Multi-Family with 5+ Units': 'MF5P_V2'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V2'
                    , 'Single-Family Attached': 'SF_V2'
                    , 'Single-Family Detached': 'SF_V2'
                    , 'Mobile Home': 'MH_V2' },
                    
                    '1980s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' },
                    
                    '1990s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' },
                    
                    '2000s' : { 'Multi-Family with 5+ Units': 'MF5P_V3'
                    , 'Multi-Family with 2 - 4 Units': 'MF24_V3'
                    , 'Single-Family Attached': 'SF_V3'
                    , 'Single-Family Detached': 'SF_V3'
                    , 'Mobile Home': 'MH_V3' } }
    
    return segment_maping

def segment_matching(target_dict):
    matched_dict = {}
    for v in target_dict.keys():
        for bt in target_dict[v].keys():
            segment_key = bt + '_' + v
            #print(segment_key)
            matched_dict[segment_key] = target_dict[v][bt]

    return matched_dict
    
def init_phi():
    
    #phi = init_Population_Distribution()
    phi = {
    'SF_V1': 0.1,
    'SF_V2': 0.33,
    'SF_V3': 0.3,
    'MF24_V1': 0.01,
    'MF24_V2': 0.03,
    'MF24_V3': 0.03,
    'MF5P_V1': 0.02,
    'MF5P_V2': 0.06,
    'MF5P_V3': 0.05,
    'MH_V1': 0.01,
    'MH_V2': 0.03,
    'MH_V3': 0.02
    }   

    return phi
# %% Buildingd Part

def ACH50_to_Finf(ach50, P=0.97, K=0.39, F=16):
    """
    Convert blower door test results (ACH_50) to natural infiltration rate (Finf).

    This function estimates the natural infiltration rate based on the air change rate at
    50 Pascals (ACH_50) from blower door tests. It accounts for the penetration factor of
    particulate matter (P), the deposition rate (K), and the conversion factor (F) to
    adjust ACH_50 to ACH_natural.

    Parameters:
    - ACH_50 (float): Air Change per Hour at 50 Pascals from a blower door test.
    - P (float): Penetration factor representing how much particulate matter gets inside. Default is 0.97.
    - K (float): Deposition factor representing how fast particulate matter settles. Default is 0.39.
    - F (float): Conversion factor to relate ACH_50 to natural air change rate. Default is 20.

    Returns:
    - float: The natural infiltration rate (Finf).

    Notes:
    - The user of this function should consider local building codes and literature for appropriate values of P, K, and F.
    - The default values provided may not be suitable for all building types and climates.
    """
    # Convert ACH_50 to natural air change rate (ACH_natural)
    #print(P)
    ach_natural = ach50 / F
    
    # Calculate natural infiltration rate (Finf)
    Finf = (P * ach_natural) / (ach_natural + K)
    
    return Finf, ach_natural


#%% Exposure

def concentration_indoor_calculator(Finf, C_out):
    """
    Calculate the indoor concentration of a pollutant.

    This function computes the indoor concentration of a pollutant based on the
    outdoor concentration and the infiltration factor.

    Parameters:
    - Finf (float): The infiltration factor, representing the fraction of outdoor
                    air pollutants that penetrate indoors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - float: The calculated indoor concentration of the pollutant.
    """

    # Calculate the indoor concentration
    C_in = C_out * Finf

    return C_in

def mean_concentration_indoor_calculator(phi, FINF, C_out):
    """
    Calculate the mean indoor concentration of a pollutant, weighted by the
    percentage of people living in different building sections.

    This function computes the mean indoor concentration by considering the
    outdoor concentration, the infiltration factors for different building
    sections, and the distribution of people across these sections.

    Parameters:
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys represent section identifiers,
                  and values are the percentage (as a float) of people in each section.
    - FINF (dict): A dictionary mapping building sections to their infiltration
                   factors. Keys represent section identifiers, and values are
                   the infiltration factors (as floats) for each section.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - float: The mean indoor concentration of the pollutant, weighted by the
             distribution of people across building sections.

    Note: It is assumed that `concentration_indoor_calculator` is a previously
    defined function that calculates the indoor concentration for a given
    infiltration factor and outdoor concentration.
    """
    
    mean_C_in = 0  # Initialize mean indoor concentration
    C_in = {}  # Dictionary to store indoor concentrations for each section
    residences_segment = FINF.keys()  # Building sections
    
    for j in residences_segment:
        Finf = FINF[j]  # Infiltration factor for the section
        C_in[j] = concentration_indoor_calculator(Finf, C_out)  # Calculate indoor concentration
        mean_C_in += C_in[j] * phi[j]  # Weighted sum of indoor concentrations
        
    return mean_C_in


def delta_exposure_calculator(t_in, phi, FINF_baseline, FINF_intervention, C_out):
    """
    Calculate the change in exposure to a pollutant indoors due to an intervention.

    This function computes the difference in mean indoor concentrations of a pollutant
    between a baseline scenario and an intervention scenario, then calculates the 
    change in exposure using time spend indoors.

    Parameters:
    - t_in (float): The time period over which exposure is calculated, in the same units
                    as used for concentration (e.g., hours).
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys are section identifiers,
                  and values are percentages of people.
    - FINF_baseline (dict): Infiltration factors for each building section under the
                            baseline scenario. Keys are section identifiers, and values
                            are infiltration factors.
    - FINF_intervention (dict): Infiltration factors for each building section under the
                                intervention scenario. Keys are section identifiers, and
                                values are infiltration factors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - Tuple: 
        - delta_mean_C_in (float): The difference in mean indoor concentrations between
                                   the intervention and baseline scenarios.
        - delta_exposure (float): The change in exposure to the pollutant over the
                                  specified time period, calculated using the difference
                                  in mean indoor concentrations.

    Note: Assumes the existence of a `mean_concentration_indoor_calculator` function that
    calculates the mean indoor concentration for given phi, FINF, and C_out parameters.
    """
    
    # Calculate mean indoor concentrations for baseline and intervention scenarios
    mean_C_in_baseline = mean_concentration_indoor_calculator(phi, FINF_baseline, C_out)
    mean_C_in_intervention = mean_concentration_indoor_calculator(phi, FINF_intervention, C_out)
    
    # Compute the difference in mean indoor concentrations due to the intervention
    delta_mean_C_in = mean_C_in_intervention - mean_C_in_baseline
    
    # Calculate the change in exposure over the specified time period
    delta_exposure = t_in * delta_mean_C_in
    
    return delta_mean_C_in, delta_exposure

def sf_concentration_calculator( FINF_baseline, FINF_intervention, C_out):
    """
    Calculate the change in exposure to a pollutant indoors due to an intervention.

    This function computes the difference in mean indoor concentrations of a pollutant
    between a baseline scenario and an intervention scenario, then calculates the 
    change in exposure using time spend indoors.

    Parameters:
    - t_in (float): The time period over which exposure is calculated, in the same units
                    as used for concentration (e.g., hours).
    - phi (dict): A dictionary mapping building sections to the percentage of
                  people living in each section. Keys are section identifiers,
                  and values are percentages of people.
    - FINF_baseline (dict): Infiltration factors for each building section under the
                            baseline scenario. Keys are section identifiers, and values
                            are infiltration factors.
    - FINF_intervention (dict): Infiltration factors for each building section under the
                                intervention scenario. Keys are section identifiers, and
                                values are infiltration factors.
    - C_out (float): The outdoor concentration of the pollutant.

    Returns:
    - Tuple: 
        - delta_mean_C_in (float): The difference in mean indoor concentrations between
                                   the intervention and baseline scenarios.
        - delta_exposure (float): The change in exposure to the pollutant over the
                                  specified time period, calculated using the difference
                                  in mean indoor concentrations.

    Note: Assumes the existence of a `mean_concentration_indoor_calculator` function that
    calculates the mean indoor concentration for given phi, FINF, and C_out parameters.
    """
    
    alt_phi = {
    'SF_V1': 0.137,
    'SF_V2': 0.452,
    'SF_V3': 0.411,
    }   
    
    # Calculate mean indoor concentrations for baseline and intervention scenarios
    mean_C_in_baseline_V1 = FINF_baseline['SF_V1'] * C_out
    mean_C_in_intervention_V1 = FINF_intervention['SF_V1'] * C_out
    
    mean_C_in_baseline_V2 = FINF_baseline['SF_V2'] * C_out
    mean_C_in_intervention_V2 = FINF_intervention['SF_V2'] * C_out
    
    mean_C_in_baseline_V3 = FINF_baseline['SF_V3'] * C_out
    mean_C_in_intervention_V3 = FINF_intervention['SF_V3'] * C_out
    
    # Compute the difference in mean indoor concentrations due to the intervention
    delta_mean_C_in_V1 = mean_C_in_intervention_V1 - mean_C_in_baseline_V1
    delta_mean_C_in_V2 = mean_C_in_intervention_V2 - mean_C_in_baseline_V2
    delta_mean_C_in_V3 = mean_C_in_intervention_V3 - mean_C_in_baseline_V3
    
    delta_mean_C_in = { 'SF_V1' : delta_mean_C_in_V1, 'SF_V2' : delta_mean_C_in_V2, 'SF_V3' : delta_mean_C_in_V3 }
    mean_C_in_baseline = { 'SF_V1' : mean_C_in_baseline_V1, 'SF_V2' : mean_C_in_baseline_V2, 'SF_V3' : mean_C_in_baseline_V3 }
    mean_C_in_intervention = { 'SF_V1' : mean_C_in_intervention_V1, 'SF_V2' : mean_C_in_intervention_V2, 'SF_V3' : mean_C_in_intervention_V3 }
    
    return delta_mean_C_in, mean_C_in_baseline, mean_C_in_intervention

def baseline_exposure_calculator( FINF, Cout, phi):
    #Should be completed based on need
    return 0

def intervention_exposure_calculator( FINF, Cout, phi, Adaptation_Impact):
    #Should be completed based on need
    return 0
'''

def Adaptation_Exposure( FINF, Cout, phi, Adaptation_Impact):
    BT_list = FINF.keys()
    AE = 0
    phi_bt_mapping = { 'Single-Family Detached': 'SDA', 'Single-Family Attached': 'SDA', 'Multi-Family with 2 - 4 Units':'2-4', 'Multi-Family with 5+ Units':'5P', 'Mobile Home':'M'}

    for BT in BT_list:
        phi_bt = phi_bt_mapping[BT]
        #Temporary
        if pd.isna(FINF[BT]):
            #print("ACH50 is empty or NaN.")
            FINF[BT] = 0
        
        #Temporary
        
        Cin = FINF[BT] * Cout * Adaptation_Impact
        if BT == 'Single-Family Attached':
            AE += 0
        else:
            AE += Cin * phi[phi_bt]
        
    return AE
'''

# %% Health Model


def psi_modifier(t_r=0, t_out=0, t_v=0, t_other=0, F_r=0, F_v=0, F_other=0):
    """
    Calculate the exposure modifier (psi) based on the time spent and infiltration factors in various microenvironments.

    The psi modifier represents the relative impact of different environments on an individual's
    overall exposure to outdoor pollutants, given the infiltration factors of those environments
    and the proportion of time spent in each.

    Parameters:
    - t_r (float): Fraction of time spent in residence.
    - t_out (float): Fraction of time spent outdoors.
    - t_v (float): Fraction of time spent in vehicles.
    - t_other (float): Fraction of time spent in other environments.
    - F_r (float): Infiltration factor for residence, the main focus of this study.
    - F_v (float): Infiltration factor for vehicles.
    - F_other (float): Infiltration factor for other environments.

    Returns:
    - float: The exposure modifier (psi) reflecting the weighted impact of time spent and infiltration across environments.

    Note:
    - The psi is dimensionless and provides a way to adjust exposure estimates to reflect time-activity patterns.
    """
    # Calculate the exposure modifier (psi)
    #Should be adjusted further
    #psi = t_r / (t_out + t_v * F_v + t_other * F_other + t_r * F_r)
    #psi = (0.7 / 0.6)
    psi = 1.248
    #psi = 1
    return psi


def HF_1(Y_0, RR, dC_in, psi):
    """
    Calculate the change in risk of mortality based on a linear concentration-response function.

    This function uses the baseline mortality rate (BMR), relative risk (RR),
    change in indoor concentration of a pollutant (dC_in), and a proportionality
    constant (psi) to estimate the change in risk of mortality (dY).

    Parameters:
    - Y_0 (float): Baseline mortality rate (BMR), representing the initial risk of mortality.
    - RR (float): Relative risk associated with exposure to the pollutant.
    - dC_in (float): Change in indoor concentration of the pollutant.
    - psi (float): Proportionality constant linking the change in concentration to the change in risk.

    Returns:
    - float: The estimated change in risk of mortality (dY) due to the change in pollutant concentration.

    Notes:
    - The attributable fraction (AF) is calculated as (RR - 1) / RR.
    - The change in risk of mortality (dY) is calculated as Y_0 * AF * dC_in * psi.
    """
    
    # Calculate the attributable fraction (AF) based on relative risk (RR)
    AF = (RR - 1) / RR
    
    # Calculate the change in risk of mortality (dY)
    dY = Y_0 * AF * (dC_in/10) * psi
    
    return dY


def HF_2(Y_0, beta, dC_in, psi):
    """
    Calculate the change in health outcome using an exponential concentration-response function.

    This function uses the baseline health outcome rate (Y_0), the coefficient beta (β),
    the change in concentration (dC_in), and the exposure duration adjustment (psi),
    to estimate the change in health outcome (ΔY) using an exponential CRF.

    Parameters:
    - Y_0 (float): Baseline health outcome rate.
    - beta (float): The coefficient representing the strength of association between the pollutant
                    concentration change and the health outcome change.
    - dC_in (float): The change in indoor concentration of the pollutant.
    - psi (float): The exposure duration adjustment factor.

    Returns:
    - float: The estimated change in health outcome (ΔY).

    Notes:
    - The equation used is ΔY = Y_0 * (e^(β * ΔC_residence,mean,a * ψ) - 1).
    """
    # Calculate the change in health outcome (ΔY) using the exponential CRF
    dY = Y_0 * (np.exp(beta * dC_in * psi) - 1)
    
    return dY

def HF_3(Y_0, beta, dC_in, psi):
    """
    Calculate the change in health outcome using an exponential concentration-response function.

    This function uses the baseline health outcome rate (Y_0), the coefficient beta (β),
    the change in concentration (dC_in), and the exposure duration adjustment (psi),
    to estimate the change in health outcome (ΔY) using an exponential CRF.

    Parameters:
    - Y_0 (float): Baseline health outcome rate.
    - beta (float): The coefficient representing the strength of association between the pollutant
                    concentration change and the health outcome change.
    - dC_in (float): The change in indoor concentration of the pollutant.
    - psi (float): The exposure duration adjustment factor.

    Returns:
    - float: The estimated change in health outcome (ΔY).

    Notes:
    - The equation used is ΔY = Y_0 * (e^(β * ΔC_residence,mean,a * ψ) - 1).
    """
    # Calculate the change in health outcome (ΔY) using the exponential CRF
    dY = -Y_0 * (np.exp(-beta * dC_in * psi) - 1)
    
    return dY

def delta_mortality_calculator(dY, Pop):
    """
    Calculate the change in the number of mortalities based on the change in risk and population size.

    Parameters:
    - dY (float): The change in risk of mortality.
    - Pop (int/float): The population size affected by the change in risk.

    Returns:
    - float: The change in the number of mortalities (dMort).
    """
    dMort = dY * Pop
    return dMort

def health_benefit_calculator(dMort, VSL):
    """
    Calculate the health benefits in monetary terms based on the change in mortality and the value of a statistical life (VSL).

    Parameters:
    - dMort (float): The change in the number of mortalities.
    - VSL (float): The value of a statistical life, which is the monetary value assigned to a change in the risk of mortality.

    Returns:
    - float: The total health benefits in monetary terms.
    """
    benefits = dMort * VSL
    return benefits

#%%
'''
def adaptation_cost_function(ach_reduction, a = 0.7848, b = 0.1891, al = 0.1416, bl = 0.4895, au = 0.303, bu = 2.0187 ):
    
    if ach_reduction > 0:
        cost_median = a * ach_reduction + b
        cost_min = al * np.log( ach_reduction ) + bl
        cost_max = au * np.exp(bu * ach_reduction)
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    return cost_median, cost_min, cost_max


'''

'''
def adaptation_cost_function(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_mean = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_mean = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    return cost_mean, cost_min, cost_max
'''

def adaptation_cost_function(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_median = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
    cost_mode = (cost_min + cost_max)/2
    
    #USD 2019 to USD 2020 (Based on RSmean)
    cost_mode = cost_mode * 1.031
    cost_min = cost_min * 1.031
    cost_max = cost_max * 1.031
    return cost_mode, cost_min, cost_max

def adaptation_cost(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_mode, cost_sqft_l, cost_sqft_u = adaptation_cost_function( ach_reduction )

    cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)

    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft[0] * floor_area
    else:
        cost = 0
    
    #print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population

def adaptation_cost_function_deter(ach_reduction, a = 0.0085, b = 0.1693, al = 0.0044, bl = 0.16, au = 0.0104, bu = 0.266 ):
    
    if ach_reduction > 0:
        ach_reduction = ach_reduction * 100
        cost_median = a * ach_reduction + b
        cost_min = al * ach_reduction + bl
        cost_max = au * ach_reduction + bu
    else:
        cost_median = 0.000005
        cost_min = 0
        cost_max = 0.00001
    
   # cost_mode = (cost_min + cost_max)/2
    
    #USD 2019 to USD 2020 (Based on RSmean)
    cost_median = cost_median * 1.031
    cost_min = cost_min * 1.031
    cost_max = cost_max * 1.031
    return cost_median, cost_min, cost_max

def adaptation_cost_deter(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_median, cost_sqft_l, cost_sqft_u = adaptation_cost_function_deter( ach_reduction )

    #cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)
    cost_sqft = cost_sqft_median
    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft * floor_area
    else:
        cost = 0
    
    #print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population

'''
def adaptation_cost(ach_baseline, ach_intervention, grid_population, phi, floor_area, occupamcy):

    ach_reduction = ((ach_baseline - ach_intervention) / ach_baseline)
    cost_sqft_mode, cost_sqft_l, cost_sqft_u = adaptation_cost_function( ach_reduction )
    #cost_sqft_mode = cost_sqft_m
    #cost_sqft_mode = 3*cost_sqft_m - cost_sqft_l - cost_sqft_u
    #print(ach_reduction, cost_sqft_mode, cost_sqft_m, cost_sqft_l, cost_sqft_u)
    #if (cost_sqft_mode < cost_sqft_l) or (cost_sqft_mode > cost_sqft_u):
    #    print("Yes")
    #    cost_sqft_mode = cost_sqft_m
    cost_sqft = np.random.triangular(left=cost_sqft_l, mode=cost_sqft_mode, right=cost_sqft_u, size=1)

    #cost = cost_sqft[0] * floor_area * ach_reduction
    if ach_reduction > 0:
        cost = cost_sqft[0] * floor_area
    else:
        cost = 0
    
    print(ach_reduction, ' :fl: ', floor_area, ' :cost: ', cost)
    cost_individual = cost / occupamcy
    cost_population = cost_individual * grid_population * phi
    return cost_population
    #return cost_individual, cost_population
'''
#%% NPV Part
def PV_Convertor(Interest_Rate, Base_Year, Secondary_Year, Value):
    
    #Interest Rate in percentage : Interest_Rate = 10
    
    Delta_Year = Secondary_Year - Base_Year
    PV = Value / (1 + (Interest_Rate/100)) ** Delta_Year
    return PV


def NPV_Calculation( PV_Cost, PV_Benefit):
    NPV = PV_Benefit - PV_Cost
    return NPV

def GRID_NPV():
    return

#%% Result analysis functions

def national_average(target_dict, pop_dict):
    """
    Calculate the weighted national average value of a target metric based on population data.

    Parameters:
    - target_dict (dict): A dictionary with groups as keys and target values as values.
    - pop_dict (dict): A dictionary with groups as keys and corresponding population numbers as values.
    
    Returns:
    - float: The calculated national average of the target metric, weighted by the population.

    Notes:
    - It's assumed that both dictionaries have the same groups as keys and that the population data is non-negative.
    """
    
    # Initialize the accumulator for the weighted target values and total population
    national_average_value = 0
    total_pop = 0
    
    # Iterate through the groups to accumulate the weighted target values and total population
    for g in pop_dict.keys():
        # Accumulate the total population
        total_pop += pop_dict[g]
        # Accumulate the weighted target value for the group
        national_average_value += target_dict[g] * pop_dict[g]
    
    # Calculate the weighted national average
    national_average_value /= total_pop
    
    return national_average_value
#%% Statistical Distributions

def sample_truncated_normal(mu, sigma, lower=-np.inf, upper=np.inf):
    """
    Samples a single value from a truncated normal distribution.

    Parameters:
    - mu (float): Mean of the normal distribution.
    - sigma (float): Standard deviation of the normal distribution.
    - lower (float): Lower bound of the truncation, default is -infinity.
    - upper (float): Upper bound of the truncation, default is infinity.

    Returns:
    - float: A single sampled value from the specified truncated normal distribution.
    """
    # Calculate the standardized lower and upper bounds
    a, b = (lower - mu) / sigma, (upper - mu) / sigma

    # Create the truncated normal distribution
    trunc_normal = truncnorm(a, b, loc=mu, scale=sigma)

    # Sample a single value
    sample = trunc_normal.rvs(1)

    return sample[0]  # return the single sampled 


#%% Model Run


def intervention_calculator(ach50_segment_dict, g, P_d = 0.97, K_d = 0.37):
    ach50_segment_dict['SF_V1'] = 5 
    #print(P_d)
    #Finf_intervention = {key: ACH50_to_Finf(0*value + 5) for key, value in ach50_segment_dict.items()}
    Finf_intervention, ACH_intervention = {key: ACH50_to_Finf(value, P = P_d, K = K_d) for key, value in ach50_segment_dict.items()}
    return Finf_intervention, ACH_intervention
'''
def code_comply_all( ach50_segment_dict, GRID_KEY, climatezone_grid_map, P_d = 0.97, K_d = 0.37 ):
    if  (climatezone_grid_map[GRID_KEY] == 1) or (climatezone_grid_map[GRID_KEY] == 2) :
        if ach50_segment_dict['SF_V1'] > 5:
            ach50_segment_dict['SF_V1'] = 5
        if ach50_segment_dict['SF_V2'] > 5:
            ach50_segment_dict['SF_V2'] = 5
        if ach50_segment_dict['SF_V3'] > 5:
            ach50_segment_dict['SF_V3'] = 5
    else:
        if ach50_segment_dict['SF_V1'] > 3:    
            ach50_segment_dict['SF_V1'] = 3
        if ach50_segment_dict['SF_V2'] > 3:
            ach50_segment_dict['SF_V2'] = 3
        if ach50_segment_dict['SF_V3'] > 3:
            ach50_segment_dict['SF_V3'] = 3
    leakage_intervention = {key: ACH50_to_Finf(value, P = P_d, K = K_d) for key, value in ach50_segment_dict.items()}
    Finf_intervention = {key: value[0] for key, value in leakage_intervention.items()}
    ACH_intervention = {key: value[1] for key, value in leakage_intervention.items()}
    
    return Finf_intervention, ACH_intervention
'''




def run_MCS(intervention_function = intervention_calculator, HF = 'HF_3', rr_beta = 0.005826891, rr_beta_sd = 0.000962763, iterations = 10):
    #pm_dict = init_PM_Concentrations_means()
    pm_dict = start_centuryPM()
    pop_dict = init_population()
    grid_key_list = list(pop_dict.keys())
    climatezone_grid_map = init_climatezone_grid_map()
    
    # Load the distribution
    #ach50_distribution = init_ach50_mean()
    ach50_distribution = init_ACH50()
    flarea_distribution = init_floor_area()
    occupants_distribution = init_occupants()
    
    phi = init_phi()
    t_in = 0.7
    year = '2001'
    psi = psi_modifier()
    
    _, _, Y_0_dict = init_Health_Model()
    #RR = rr
    #RR_sd = rr_sd
    rr_beta = 0.0070
    rr_beta_sd = 0.0016
    list_delta_mean_C_in = []
    
    list_sf_delta_mean_C_in = []
    list_sf_baseline_mean_C_in = []
    list_sf_intervention_mean_C_in = []
    
    list_dMort = []
    list_benefit = []
    list_dY = []
    list_cost = []
    list_NB = []
    
    list_FINF0 = []
    list_FINF1 = []
    list_ACHN0 = []
    list_ACHN1 = []
    
    for n in range(iterations):
        print('iteration: ', n)
        #ach50_dict = reorganized_dict(ach50_dict)
        #floor_area_dict = init_floor_area()
        #floor_area_dict = reorganized_dict(floor_area_dict)
        #occupancy_dict = init_occupancy()
        #occupancy_dict = reorganized_dict(occupancy_dict)
        
        #Sample from the distribution
        ach50_dict = sample_ach50_mean(ach50_distribution)
        #ach50_dict = ach50_distribution
        
        flarea_dict = sample_flarea_mean(flarea_distribution)
        occupants_dict = sample_occupants_mean(occupants_distribution)
        
        delta_mean_C_in = {}
        delta_exposure = {}
        
        sf_delta_mean_C_in = {}
        sf_baseline_mean_C_in = {} 
        sf_intervention_mean_C_in = {}
        
        
        dY = {}
        dMort = {}
        benefit = {}
        cost = {}
        agg_cost = {}
        NB = {}
        
        FINF0 = {}
        FINF1 = {}
        ACHN0 = {}
        ACHN1 = {}
        
        #Azimi sptephens
        #rr_beta = 0.0070
        #rr_beta_sd = 0.0016
        
        
        sample_rr_beta = np.random.normal(rr_beta, rr_beta_sd, 1)


        x = sample_rr_beta[0]
        #x = beta
        shape = 1.509588  # This is k
        scale = 9648168   # This is λ
        VSL = np.random.weibull(shape, 1) * scale

        #VSL_2023 = VSL[0] * 1.23
        #VSL 2020 based on bls.gov CPI calculator 2015 -> 2020 (1 Jan)
        VSL_2020 = VSL[0] * 1.1
        
        #P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
        #K_random = sample_truncated_normal(0.39, 0.08, lower=0)
        
        for g in grid_key_list:
            
            ach50_segment_dict = segment_matching( ach50_dict[g]  )
            #floor area
            flarea_segment_dict = segment_matching( flarea_dict[g] )
            #occupancy
            occupants_segment_dict = segment_matching( occupants_dict[g] )
            
            '''
            Reengineering required here/; delta ACH needed for cost calculation
            ACH 0 ->  FINF BASELINE
            ACH
            '''
            #P_random = sample_truncated_normal(0.97, 0.06, lower=0, upper=1)
            P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
            K_random = sample_truncated_normal(0.39, 0.08, lower=0)
            
            #Fix
            #P_random = 0.72
            #K_random = 0.39
            
            #Dictionary Building segment to a tuple of Finf and ACH Nat
            leakage_baseline = {key: ACH50_to_Finf(value, P = P_random, K = K_random) for key, value in ach50_segment_dict.items()}
            
            #Dictionary of Finf
            FINF_baseline = {key: value[0] for key, value in leakage_baseline.items()}
        
            #Dictionary of ACH nat
            ACH_baseline = {key: value[1] for key, value in leakage_baseline.items()}
            
            #Dictionary of Finf and ACH after intervention
            FINF_intervention, ACH_intervention = intervention_function(ach50_segment_dict, g, climatezone_grid_map, P_d = P_random, K_d = K_random)
            
            FINF0[g] = FINF_baseline
            FINF1[g] = FINF_intervention
            ACHN0[g] = ACH_baseline
            ACHN1[g] = ACH_intervention
            
            cost[g] = {key: adaptation_cost(ACH_baseline[key], ACH_intervention[key], pop_dict[g], phi[key],flarea_segment_dict[key] , occupants_segment_dict[key] ) for key, value in ACH_baseline.items()} 
            #print(cost[g])
            #C_out = pm_dict[year][g]
            C_out = pm_dict[g]
            
            delta_mean_C_in[g], delta_exposure[g] = delta_exposure_calculator(t_in, phi, FINF_baseline, FINF_intervention, C_out)
            sf_delta_mean_C_in[g], sf_baseline_mean_C_in[g], sf_intervention_mean_C_in[g] = sf_concentration_calculator( FINF_baseline, FINF_intervention, C_out)
            #beta = 0.0058
            #beta_sd = 0.001
            #sample_beta = np.random.normal(beta, beta_sd)
            #print(sample_beta)

            #x = sample_beta
            if HF == 'HF_1':
                dY[g] = HF_1(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_2':
                dY[g] = HF_2(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_3':
                #print(x)
                dY[g] = HF_3(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
                
            dMort[g] = delta_mortality_calculator(dY[g], pop_dict[g])
            
        
            
            benefit[g] = health_benefit_calculator(dMort[g], VSL_2020)
            
            agg_cost[g] = sum(cost[g].values())
            NB[g] = benefit[g] - agg_cost[g]
            

        list_delta_mean_C_in.append(delta_mean_C_in)
        
        list_sf_delta_mean_C_in.append(sf_delta_mean_C_in)
        list_sf_baseline_mean_C_in.append(sf_baseline_mean_C_in)
        list_sf_intervention_mean_C_in.append(sf_intervention_mean_C_in)

        list_dMort.append(dMort)
        list_benefit.append(benefit)
        list_dY.append(dY)
        list_cost.append(agg_cost)
        list_NB.append(NB)
        list_FINF0.append(FINF0)
        list_FINF1.append(FINF1)
        list_ACHN0.append(ACHN0)
        list_ACHN1.append(ACHN1)
        
    return list_delta_mean_C_in, list_dMort, list_benefit, list_dY, list_cost, list_NB, list_FINF0, list_FINF1, list_ACHN0, list_ACHN1, list_sf_delta_mean_C_in, list_sf_baseline_mean_C_in, list_sf_intervention_mean_C_in             

#%%
#MCS Uncertainty Analysis

def run_MCS_UC_Analysis(intervention_function = intervention_calculator, HF = 'HF_3', rr_beta = 0.005826891, rr_beta_sd = 0.000962763, iterations = 10):
    #pm_dict = init_PM_Concentrations_means()
    pm_dict = start_centuryPM()
    pop_dict = init_population()
    grid_key_list = list(pop_dict.keys())
    climatezone_grid_map = init_climatezone_grid_map()
    
    # Load the distribution
    #ach50_distribution = init_ach50_mean()
    ach50_distribution = init_ACH50()
    flarea_distribution = init_floor_area()
    occupants_distribution = init_occupants()
    
    phi = init_phi()
    t_in = 0.7
    year = '2001'
    psi = psi_modifier()
    
    _, _, Y_0_dict = init_Health_Model()
    #RR = rr
    #RR_sd = rr_sd
    
    list_delta_mean_C_in = []
    
    list_sf_delta_mean_C_in = []
    list_sf_baseline_mean_C_in = []
    list_sf_intervention_mean_C_in = []
    
    list_dMort = []
    list_benefit = []
    list_dY = []
    list_cost = []
    list_NB = []
    
    list_FINF0 = []
    list_FINF1 = []
    list_ACHN0 = []
    list_ACHN1 = []
    
    for n in range(iterations):
        print('iteration: ', n)
        #ach50_dict = reorganized_dict(ach50_dict)
        #floor_area_dict = init_floor_area()
        #floor_area_dict = reorganized_dict(floor_area_dict)
        #occupancy_dict = init_occupancy()
        #occupancy_dict = reorganized_dict(occupancy_dict)
        
        #Sample from the distribution
        ach50_dict = sample_ach50_mean(ach50_distribution)
        #ach50_dict = ach50_distribution
        
        flarea_dict = sample_flarea_mean(flarea_distribution)
        occupants_dict = sample_occupants_mean(occupants_distribution)
        
        delta_mean_C_in = {}
        delta_exposure = {}
        
        sf_delta_mean_C_in = {}
        sf_baseline_mean_C_in = {} 
        sf_intervention_mean_C_in = {}
        
        
        dY = {}
        dMort = {}
        benefit = {}
        cost = {}
        agg_cost = {}
        NB = {}
        
        FINF0 = {}
        FINF1 = {}
        ACHN0 = {}
        ACHN1 = {}
        
        #Azimi sptephens
        #rr_beta = 0.0070
        #rr_beta_sd = 0.0016
        
        
        sample_rr_beta = np.random.normal(rr_beta, rr_beta_sd, 1)


        x = sample_rr_beta[0]
        #x = beta
        shape = 1.509588  # This is k
        scale = 9648168   # This is λ
        VSL = np.random.weibull(shape, 1) * scale

        #VSL_2023 = VSL[0] * 1.23
        #VSL 2020 based on bls.gov CPI calculator 2015 -> 2020 (1 Jan)
        VSL_2020 = VSL[0] * 1.1
        
        #P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
        #K_random = sample_truncated_normal(0.39, 0.08, lower=0)
        
        for g in grid_key_list:
            
            ach50_segment_dict = segment_matching( ach50_dict[g]  )
            #floor area
            flarea_segment_dict = segment_matching( flarea_dict[g] )
            #occupancy
            occupants_segment_dict = segment_matching( occupants_dict[g] )
            
            '''
            Reengineering required here/; delta ACH needed for cost calculation
            ACH 0 ->  FINF BASELINE
            ACH
            '''
            #P_random = sample_truncated_normal(0.97, 0.06, lower=0, upper=1)
            P_random = sample_truncated_normal(0.72, 0.21, lower=0, upper=1)
            K_random = sample_truncated_normal(0.39, 0.08, lower=0)
            
            #Fix
            #P_random = 0.72
            #K_random = 0.39
            
            #Dictionary Building segment to a tuple of Finf and ACH Nat
            leakage_baseline = {key: ACH50_to_Finf(value, P = P_random, K = K_random) for key, value in ach50_segment_dict.items()}
            
            #Dictionary of Finf
            FINF_baseline = {key: value[0] for key, value in leakage_baseline.items()}
        
            #Dictionary of ACH nat
            ACH_baseline = {key: value[1] for key, value in leakage_baseline.items()}
            
            #Dictionary of Finf and ACH after intervention
            FINF_intervention, ACH_intervention = intervention_function(ach50_segment_dict, g, climatezone_grid_map, P_d = P_random, K_d = K_random)
            
            FINF0[g] = FINF_baseline
            FINF1[g] = FINF_intervention
            ACHN0[g] = ACH_baseline
            ACHN1[g] = ACH_intervention
            
            #cost[g] = {key: adaptation_cost(ACH_baseline[key], ACH_intervention[key], pop_dict[g], phi[key],flarea_segment_dict[key] , occupants_segment_dict[key] ) for key, value in ACH_baseline.items()} 
            cost[g] = {key: adaptation_cost_deter(ACH_baseline[key], ACH_intervention[key], pop_dict[g], phi[key],flarea_segment_dict[key] , occupants_segment_dict[key] ) for key, value in ACH_baseline.items()} 

            #print(cost[g])
            #C_out = pm_dict[year][g]
            C_out = pm_dict[g]
            
            delta_mean_C_in[g], delta_exposure[g] = delta_exposure_calculator(t_in, phi, FINF_baseline, FINF_intervention, C_out)
            sf_delta_mean_C_in[g], sf_baseline_mean_C_in[g], sf_intervention_mean_C_in[g] = sf_concentration_calculator( FINF_baseline, FINF_intervention, C_out)
            #beta = 0.0058
            #beta_sd = 0.001
            #sample_beta = np.random.normal(beta, beta_sd)
            #print(sample_beta)

            #x = sample_beta
            #x = 0.005826891
            if HF == 'HF_1':
                dY[g] = HF_1(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_2':
                dY[g] = HF_2(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
            elif HF == 'HF_3':
                #print(x)
                dY[g] = HF_3(Y_0_dict[g], x, -delta_mean_C_in[g], psi)
                
            dMort[g] = delta_mortality_calculator(dY[g], pop_dict[g])
            
        
            #VSL_2020 = 8705114 * 1.1
            benefit[g] = health_benefit_calculator(dMort[g], VSL_2020)
            
            agg_cost[g] = sum(cost[g].values())
            NB[g] = benefit[g] - agg_cost[g]
            

        list_delta_mean_C_in.append(delta_mean_C_in)
        
        list_sf_delta_mean_C_in.append(sf_delta_mean_C_in)
        list_sf_baseline_mean_C_in.append(sf_baseline_mean_C_in)
        list_sf_intervention_mean_C_in.append(sf_intervention_mean_C_in)

        list_dMort.append(dMort)
        list_benefit.append(benefit)
        list_dY.append(dY)
        list_cost.append(agg_cost)
        list_NB.append(NB)
        list_FINF0.append(FINF0)
        list_FINF1.append(FINF1)
        list_ACHN0.append(ACHN0)
        list_ACHN1.append(ACHN1)
        
    return list_delta_mean_C_in, list_dMort, list_benefit, list_dY, list_cost, list_NB, list_FINF0, list_FINF1, list_ACHN0, list_ACHN1, list_sf_delta_mean_C_in, list_sf_baseline_mean_C_in, list_sf_intervention_mean_C_in             
